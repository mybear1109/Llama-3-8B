# ğŸ“„ pages/ì¦ìƒì„¤ëª….py
import streamlit as st
import json
from huggingface_hub import InferenceClient
import toml
import os
import re

# âœ… API í‚¤ ë¡œë”©
toml_path = os.path.join(os.path.dirname(__file__), "..", ".streamlit", "secrets.toml")
secrets = toml.load(toml_path)
HF_API_KEY = secrets["huggingface"]["HUGGINGFACE_API_TOKEN"]

# âœ… HuggingFace LLaMA ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸
client = InferenceClient(
    model="meta-llama/Meta-Llama-3.1-8B-Instruct",
    token=HF_API_KEY
)

st.header("ğŸ’¡ ì˜ì‹¬ ì¦ìƒ ë° ì„¤ëª…")

user_input = st.text_input("ì¦ìƒì„ ì…ë ¥í•˜ì„¸ìš”:", key="ì¦ìƒì…ë ¥")

# ì½”ë“œë¸”ë¡ ì œê±° í•¨ìˆ˜
def remove_code_blocks(text: str) -> str:
    return re.sub(r"```(?:json|python)?\\n?|```", "", text).strip()

if st.button("ì¦ìƒ ë¶„ì„") and user_input:
    prompt = f"""
ë‹¤ìŒ ë¬¸ì¥ì„ ë³´ê³  ì£¼ìš” ì¦ìƒ ëª©ë¡ê³¼ ì„¤ëª…ì„ JSONìœ¼ë¡œ ì¶œë ¥í•´ì¤˜.
í˜•ì‹:
{{
  "ì¦ìƒ": [
    {{"ì´ë¦„": "ë‘í†µ", "ì„¤ëª…": "ë¨¸ë¦¬ê°€ ì•„í”ˆ ì¦ìƒì…ë‹ˆë‹¤."}},
    {{"ì´ë¦„": "êµ¬í† ", "ì„¤ëª…": "ìŒì‹ì„ í† í•´ë‚´ëŠ” ì¦ìƒì…ë‹ˆë‹¤."}}
  ]
}}
ë¬¸ì¥: "{user_input}"
ì½”ë“œë¸”ë¡ ì—†ì´ JSONë§Œ ì¶œë ¥í•´ì¤˜. ì„¤ëª… ì—†ì´.
"""
    try:
        response = client.text_generation(prompt=prompt, max_new_tokens=384)
        cleaned = remove_code_blocks(response)
        result = json.loads(cleaned)
        ì¦ìƒë“¤ = result.get("ì¦ìƒ", [])

        if not ì¦ìƒë“¤:
            st.warning("â— ì¦ìƒ ì •ë³´ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            for ì¦ in ì¦ìƒë“¤:
                st.markdown(f"- **{ì¦['ì´ë¦„']}**: {ì¦['ì„¤ëª…']}")

    except Exception as e:
        st.error(f"â— ì¦ìƒ ì„¤ëª… ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        st.text_area("AI ì›ë¬¸ ì‘ë‹µ", response)