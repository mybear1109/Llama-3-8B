# ğŸ“„ pages/ì§„ë£Œê³¼ì¶”ì²œ.py
import streamlit as st
import toml
import os
import json
import re
from huggingface_hub import InferenceClient

# âœ… secrets.toml ê²½ë¡œ ì§€ì • ë° API í‚¤ ë¡œë”©
toml_path = os.path.join(os.path.dirname(__file__), "..", ".streamlit", "secrets.toml")
secrets = toml.load(toml_path)
HF_API_KEY = secrets["huggingface"]["HUGGINGFACE_API_TOKEN"]

# âœ… LLaMA ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = InferenceClient(
    model="meta-llama/Meta-Llama-3.1-8B-Instruct",
    token=HF_API_KEY
)

st.header("ğŸ¥ ì¦ìƒ ê¸°ë°˜ ì§„ë£Œê³¼ ì¶”ì²œ")

user_input = st.text_input("ì–´ë–¤ ì¦ìƒì´ ìˆìœ¼ì‹ ê°€ìš”?", key="ì§„ë£Œê³¼ì…ë ¥")

# ì½”ë“œë¸”ë¡ ì œê±° ìœ í‹¸ í•¨ìˆ˜
def remove_code_blocks(text: str) -> str:
    return re.sub(r"```(?:json|python)?\\n?|```", "", text).strip()

if st.button("ì§„ë£Œê³¼ ë¶„ì„ ì‹œì‘") and user_input:
    prompt = f"""
ë‹¤ìŒ ë¬¸ì¥ì„ ë³´ê³  ì ì ˆí•œ ì§„ë£Œê³¼ 1~2ê°œë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì¤˜.
ë¬¸ì¥: "{user_input}"
ì¶œë ¥ í˜•ì‹:
{{
  "ì§„ë£Œê³¼": ["ë‚´ê³¼", "ì‹ ê²½ê³¼"]
}}
ë°˜ë“œì‹œ ìœ„ì™€ ê°™ì€ JSONë§Œ ì¶œë ¥í•˜ê³  ì½”ë“œë¸”ëŸ­ ì—†ì´ ì¶œë ¥í•´ì¤˜.
"""

    try:
        response = client.text_generation(prompt=prompt, max_new_tokens=256)
        cleaned = remove_code_blocks(response)
        result = json.loads(cleaned)

        ì§„ë£Œê³¼ = result.get("ì§„ë£Œê³¼", [])
        if ì§„ë£Œê³¼:
            st.markdown("## ğŸ·ï¸ ì¶”ì²œ ì§„ë£Œê³¼")
            for idx, dept in enumerate(ì§„ë£Œê³¼, 1):
                st.success(f"**ì¶”ì²œ {idx}: {dept}**")
        else:
            st.warning("â— ì§„ë£Œê³¼ ì •ë³´ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    except Exception as e:
        st.error(f"â— ì§„ë£Œê³¼ ë¶„ì„ ì‹¤íŒ¨: {e}")
        st.text_area("AI ì›ë¬¸ ì‘ë‹µ", response)