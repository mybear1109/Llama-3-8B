# pages/ì§ˆí™˜ì •ë³´.py

import streamlit as st
import json
from huggingface_hub import InferenceClient
import toml
import os
import re

# âœ… API í‚¤ ë¡œë”©
toml_path = os.path.join(os.path.dirname(__file__), "..", ".streamlit", "secrets.toml")
secrets = toml.load(toml_path)
HF_API_KEY = secrets["huggingface"]["HUGGINGFACE_API_TOKEN"]

client = InferenceClient(model="meta-llama/Meta-Llama-3.1-8B-Instruct", token=HF_API_KEY)

st.header("ğŸ§¬ ê´€ë ¨ ì§ˆí™˜ ì„¤ëª…")

user_input = st.text_input("ì¦ìƒì„ ì…ë ¥í•˜ì„¸ìš”:", key="ì§ˆí™˜ì…ë ¥")

if st.button("ê´€ë ¨ ì§ˆí™˜ ë³´ê¸°") and user_input:
    prompt = f"""
ë‹¤ìŒ ì¦ìƒì—ì„œ ê´€ë ¨ ì§ˆí™˜ 1~2ê°œì™€ ì„¤ëª…ì„ JSONìœ¼ë¡œ ì¶œë ¥í•´ì¤˜.
í˜•ì‹:
{{"ê´€ë ¨ ì§ˆí™˜": [{{"ì´ë¦„": "í¸ë‘í†µ", "ì„¤ëª…": "ì¼ì¸¡ì„± ë‘í†µìœ¼ë¡œ..."}}, ...]}}
ë¬¸ì¥: "{user_input}"
JSONë§Œ ì¶œë ¥í•˜ê³  ì„¤ëª…í•˜ì§€ ë§ˆ.
"""

    try:
        response = client.text_generation(prompt=prompt, max_new_tokens=384)
        response = re.sub(r"```.*?\\n|```", "", response.strip())
        result = json.loads(response)
        ì§ˆí™˜ë“¤ = result.get("ê´€ë ¨ ì§ˆí™˜", [])

        for ì§ˆí™˜ in ì§ˆí™˜ë“¤:
            st.markdown(f"- **{ì§ˆí™˜['ì´ë¦„']}**: {ì§ˆí™˜['ì„¤ëª…']}")

    except Exception as e:
        st.error(f"â— ê´€ë ¨ ì§ˆí™˜ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")