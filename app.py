import streamlit as st
import toml
import os
from huggingface_hub import InferenceClient
from symptoms import classify_symptoms

# âœ… secrets.tomlì— ì €ì¥ëœ ê°’ ë¶ˆëŸ¬ì˜¤ê¸°
secrets_path = os.path.join(os.path.dirname(__file__), ".streamlit", "secrets.toml")
secrets = toml.load(secrets_path)
HF_API_KEY = secrets["huggingface"]["HUGGINGFACE_API_TOKEN"]

# LLaMA ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸
client = InferenceClient(
    provider="novita",
    api_key=HF_API_KEY
)

# Streamlit UI
st.set_page_config(page_title="AI ì§„ë£Œ ë„ìš°ë¯¸", layout="centered")
st.title("ğŸ©º AI ì§„ë£Œ ë„ìš°ë¯¸")
st.markdown("ì¦ìƒì„ ì…ë ¥í•˜ë©´ ì§„ë£Œê³¼ì™€ ì¦ìƒ ìš”ì•½ì„ AIê°€ ë¶„ì„í•´ì¤ë‹ˆë‹¤.")

if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "system", "content": "ë„ˆëŠ” ì¦ìƒì—ì„œ ì§„ë£Œê³¼ì™€ ì£¼ìš” ì¦ìƒì„ ìš”ì•½í•´ì£¼ëŠ” AIì•¼. ê²°ê³¼ëŠ” JSONìœ¼ë¡œ ì•Œë ¤ì¤˜."}
    ]

user_input = st.text_input("ì¦ìƒì„ ì…ë ¥í•˜ì„¸ìš”:", "")

if st.button("AI ë¶„ì„ ìš”ì²­") and user_input:
    # ë¡œì»¬ ì‚¬ì „ ê¸°ë°˜ ì§„ë‹¨
    local_result = classify_symptoms(user_input)
    st.write("ğŸ“‹ **ë¡œì»¬ ì§„ë‹¨ ê²°ê³¼**")
    st.json(local_result)

    # AI ëŒ€í™” ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": user_input})
    completion = client.chat.completions.create(
        model="meta-llama/Meta-Llama-3.1-8B-Instruct",
        messages=st.session_state.messages,
        max_tokens=512,
    )
    ai_response = completion.choices[0].message["content"]
    st.session_state.messages.append({"role": "assistant", "content": ai_response})

    # ê²°ê³¼ ì¶œë ¥
    st.write("ğŸ¤– **AI ì‘ë‹µ**")
    st.code(ai_response, language="json")
